{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rod Pump Failure: K-Nearest Neighbors Model, Iteration 2\n",
    "**Improvements:**\n",
    "* Added grid search for `n_neighbors` hyperparameter\n",
    "* Added cross validation in model training\n",
    "* Added data balancing function eliminate binning of output values\n",
    "* Tested classifier on entire dataset to ensure successful implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "from floridaman import data_cleaning\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "raw_data = data_cleaning.load('null_transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate candidate dataset by removing nulls from raw_data using parameters\n",
    "# Drop columns with more than 40% null values\n",
    "# Drop rows with between 40% and 40% null values\n",
    "# Impute rows with missing data below 40% null values using 5 n_neighbors\n",
    "\n",
    "candidate_data = data_cleaning.generate_candidate_dataset(raw_data, .40, .40, 5)\n",
    "\n",
    "# Create training dataset by balancing the number of occurances of each observed failuretype\n",
    "\n",
    "train_data = data_cleaning.balance(candidate_data)\n",
    "\n",
    "# Split training data into X and y\n",
    "\n",
    "X_train = np.array(train_data[data_cleaning.features(train_data)])\n",
    "y_train = np.array(train_data['FAILURETYPE'])\n",
    "\n",
    "# Split all data into X and y for testing\n",
    "\n",
    "X_test = np.array(candidate_data[data_cleaning.features(candidate_data)])\n",
    "y_test = np.array(candidate_data['FAILURETYPE'])\n",
    "\n",
    "# Create KNN classifier and parameter grid to test 1-30 n_neighbors\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': np.arange(1, 100)}\n",
    "\n",
    "# Create grid search object with specified parameter grid\n",
    "# using roc_auc_ovo scoring and cross validation with 8 folds\n",
    "\n",
    "clf = GridSearchCV(knn, param_grid, scoring='roc_auc_ovo', cv=8, n_jobs=12)\n",
    "\n",
    "# Fit classifier to X_train and y_train\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print some results from training and testing data\n",
    "\n",
    "#print(\"Scoring method: roc_auc_ovo\")\n",
    "#print(\"Classifier training score: \" + str(clf.best_score_)) # prints best roc score from training set\n",
    "#print(\"Classifier testing score: \" + str(clf.score(X_test, y_test))) # prints best roc score using all data as test\n",
    "#print(\"Best parameters: \" + str(clf.best_params_)) # prints ideal values for parameters\n",
    "\n",
    "print(str(classification_report(y_test, clf.predict(X_test))))\n",
    "# Plot the confusion matrix\n",
    "\n",
    "conf_mat = plot_confusion_matrix(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "knn = KNeighborsClassifier(clf.best_params_['n_neighbors'])\n",
    "knn.fit(X_train,y_train)\n",
    "# Produce the SHAP values\n",
    "knn_explainer = shap.KernelExplainer(knn.predict_proba,shap.sample(X_test,1))\n",
    "knn_shap_values = knn_explainer.shap_values(X_test)\n",
    "shap.save_html('KNNShap.html',shap.force_plot(knn_explainer.expected_value[0], knn_shap_values[0], X_test, matplotlib = False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
